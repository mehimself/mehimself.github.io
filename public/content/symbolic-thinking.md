# Symbolic Reasoning

*June 21, 2025*

When LLMs reason, they do so as disembodied mechanic minds with a fractured representation of our complex world. Large chunks of their internalized knowledge match reasonably well with our reality, while gaps in knowledge are filled with assumptions and rationalizations.  

## Why fragmeted?

LLMs are not efficient at capturing facts in a retrievable fashion. Estimates range from 100-1000 parameters required per persistently memorized fact. Another practical limit to model-based memory is trust. In his recent keynote at San Francisco Startup School Andrej Karpathy [describes LLM intelligence as *jagged intelligence*](https://youtu.be/LCEmiRjPEtQ?si=nl50N-IIirOuKw3B&t=980). LLM generations are useful, but users can not reliably verify if a fact has been retrieved from persistent memory, or rehydrated (read: hallucinated) from a generalization, or projected through rationalization. 

## How does fragmentation impede reasoning?
When a model reasons it traces a pathways from a problem to candidate solutions. These traces rely on its understanding of our world. Our Laws, conventions, practices and exceptions and the messy multitude of divergent perspectives comprise its model of our world. The better a model's *formal grounding* (micro- and macro alignment with our world), the more accurate and complete its representation of our world is, the more efficient model reasoning will be.

Any discrepancy will cause errors that compound as reasoning progresses. This causes debilitating defects whenever a misaligned detail is considered in the latent space of a model's reasoning.    

## Reasoning Paths
While there a many different patterns in which reasoning progresses, we can use one basic pattern to illustrate the underlying mechanism. Reasoning often poses problems based on the pattern `what is A to B as X is to Y`. This pattern enters the world model at a specific triplet `(X)-[effect]->(Y)` and to find which unkown `A` affords the same `effect` on `B`. What `X` and `A` have in common is an abstraction in the form of a transient commonality. If `A` and `effect` exist we will find short associative paths between `X` and `A`. Efficient reasoning will then search paths breadth-first from `B` until `effect` is found. 

Any discrepancy between our world and a world model along this reasoning path will cripple reasoning (by producing false positives and false negatives). As mentioned above, LLMs tend to compensate by completing a response with hallucinated claims projecting unviable or even non-existing reasoning paths. 

## Why does it matter? 
A *formallu grounded* *world model* will have much fever defects and allow much longer reasoning chains, more precise summaries, and powerful symbolic representations of a complex issues. This applies to both training and inference. Training could becomee much more efficient where simple 'surprise-algorithms' could allow the capture of a fact about the world in a simple encounter. In agentic reasoning architectures the model iterates between memory and attention management steps. Both require an overhead for stabilization and error correction.    

Attention costs would decline, because large contexts could be efficiently compressed without the need for quadratically scaling training exposure.
Reasoning chains could be much longer with lower error correction overhead. Simultaneously, a grounded model would have internalized laws as basic symbolic intermediary graphs - finetuning and expert delegation could be greatly minimized or even abolished. The intimate grasp of physics and mathmatics as symbolic foundation for STEM work would make reasoning very reliable, efficient, and scalable. While there are complicating dimensions to grounding any prpogress in this matter likely produces exponential gronwth in model intelligence.


## The limits of alignment 
When a model is fractured its reasoning becomes inconsistent, because cause-effect chains are not leading to the same 

## How to align a model with our world
_one additional step of common sense_

Working memory should not be black boxed since the provenance of details used for reasoning and tasks need to remain traceable for synergistic human-centered collaboration. The alternative could be browseable ***knowledge graphs*** with shards made up of entities and their prolific relationships in a graph. The graph nodes and vertices are generated by generative shard definitions comprised of verbatim quote / name, contextual definition and if applicable a 'canonical' abstraction supplied by the LLM.

The purpose of the canonical abstraction is to ground idiosyncratic terms while maintaining overloaded definitions in locally relevant contexts. By asking the LLM what does this mean in context and what are features called in other contexts, generated shard definitions draw on the model's internal abstractions, which also are used in reasoning chains.

This multi-resolution of terms in local context and global contexts reflects the model's understanding of our world, adding a rhizome of abstraction relationships aligning the embeddings of our knowledge shards with common sense-making patterns, conventions, laws, and practices.
